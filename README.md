# 🧠 Supplementary Material — US-Prompt Study

This repository contains the supplementary material for the article:  
*"AI-Generated User Stories: Are They Good Enough?"*  
Accepted at the 39th Brazilian Symposium on Software Engineering (SBES 2025) – Track on Innovative Ideas and Emerging Results.

This study investigates the automated generation of user stories using Large Language Models (LLMs), with a particular focus on US-Prompt, an advanced prompting approach that combines Meta Prompting and Few-Shot Prompting.

## 📁 File Descriptions

### 📊 Analysis of Modifications.xlsx
Contains the analysis of improvements made by each group to the user stories. The spreadsheet compares the versions generated using the standard US-Prompt with those produced after specific modifications made by each group, highlighting the impact of the changes on story quality.

### 📊 Evaluation of User Story.xlsx
Includes the quality evaluation of user stories generated by participants based on the *Quality User Story (QUS)* framework. Each story was analyzed according to criteria such as atomicity, completeness, and estimability.

### 📌 Quality User Story Framework
An explanatory document presenting the seven criteria of the *QUS* framework (Lucassen et al., 2016). It serves as the basis for assessing story quality, ensuring that they are well-defined, understandable, and useful for agile development.

### 📊 TAM Questionnaire.xlsx
A questionnaire based on the *Technology Acceptance Model (TAM)*, applied to assess participants' acceptance of the automated approach. It measures perceived usefulness, ease of use, and behavioral intention to use the technology.

### 📌 US-Prompt for User Story
A document containing the Standard US-Prompts used for the automated generation of user stories. It includes all activities and instructions to guide the production of high-quality stories.
